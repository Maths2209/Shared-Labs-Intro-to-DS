---
title: "Lab8"
author: "Dion, Julie, Mia, Vincent"
output: 
    prettydoc::html_pretty:
    theme: cayman
    highlight: github
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(rpart.plot)
library(e1071)
```

This lab's data are derived from the spambase dataset from the UCI Machine Learning Repository. The data are posted as the CSV file spam.csv and include whether an email was spam (type) and for 8 words whether they were present in the email or not. For instance, the variable george is TRUE if the email contained 'george' and FALSE otherwise.

```{r}
spam <- read.csv("spam.csv")
```


# Exercise 1: Fit a naive Bayes classifier with type as outcome and all other variables as predictors. Your R output should contain similar information on the naive Bayes classifier as on page 222 of the textbook.

```{r}
traindata <- as.data.frame(spam)

#build one model without smoothing 
model <- naiveBayes(type ~ .,traindata)
model

#build one model with smoothing 
model2 <- naiveBayes(type ~ .,traindata,  laplace=.01)
model2
```


# Exercise 2: Use your naive Bayes classifier from Exercise 4 to predict whether the 300th email is spam or not.
```{r}

testdata <- traindata[300,]

#predict using the model without smoothing 
results <- predict (model,testdata)
results 

#predict using the model with smoothing 
results2 <- predict (model2,testdata)
results2 

```
Based on both models, it is predicted to be not a spam. 

# Exercise 3: What word was most influential in determining the outcome from Exercise 2? Use the output from Exercise 1.

The absence ofthe word 'conference' is most influential. P(nonspam|conferenceFalse) is the highest compared to the relevant conditional probabilities for other predictors. 

# Exercise 4: Fit a decision tree using rpart and plot it. Use `rpart`â€™s default parameters, that is `control=rpart.control(minsplit=20, cp = .01)`.

```{r}
fit1 <- rpart(
  type ~ .,
  data = spam,
  method = "class",
  control=rpart.control(minsplit=20, cp = .01),
  parms=list(split='information')
)
rpart.plot(fit1)
```


# Exercise 5: What is the depth of this tree? How many words, at most, does this decision tree consider before classifying an email as spam? How many words does your naive Bayes classifier from Exercise 1 consider?

Depth of tree = 4. This decision tree considers, at most, 4 words before classifying an email as spam. 

