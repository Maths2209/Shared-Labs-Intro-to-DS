---
title: "Lab7"
author: "Dion, Julie, Mia, Vincent"
date: "9/20/2019"
output: 
    prettydoc::html_pretty:
    theme: cayman
    highlight: github
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
<<<<<<< Updated upstream
=======
library(FSelectorRcpp)
library(dplyr)
>>>>>>> Stashed changes
```


This lab considers the Kaggle dataset Mall Customer Segmentation Data https://www.kaggle.com/vjchoudhary7/customer-segmentation-tutorial-in-python which is posted as Mall_Customers.csv on Canvas.

For Exercises 1 through 3, consider whether the spending score is greater than 50 as outcome. Use the natural logarithm in your computations, not the log with base 2 like in the textbook. The R function log computes the natural logarithm by default.


```{r}
data <- read.csv("Mall_customers.csv")
names(data)[4:5] <- c("Annual.Income", "Spending.Score")
```

general notes:
- function that computes the entropy --> conditional entropy, IG
- entropy ranges from 0 to log(n), n = no. of outcomes

# Exercise 1: Compute the information gain in the outcome due to knowing whether someone is older than 36 or not.

First, we create a function that computes entropy, $H_Y$:
```{r}
entropy_func <- function(Y) {
  probs <- prop.table(table(Y))
  n <- length(probs)
  result <- rep_len(0, n)
  for (y in seq_len(n)) {
    result[y] <- probs[y] * log(probs[y])
  }
  -sum(result)
}
```


Now, we must compute conditional entropy, $H_{Y \vert X}$. To compute this, we first obtain the probability of the condition (someone being older than 36), $P(X)$.
```{r}
table(data$Age > 36) / length(data$Age)
```







```{r}




data %>%
  summarize(prob = sum(Age > 36 & Spending.Score == 2) / sum(Age > 36))


# prob age > 36
length(which(data$Age > 36)) / length(data$Age)

# prob age <= 36
length(which(data$Age <= 36)) / length(data$Age)


```


# Exercise 2: Compute the information gain in the outcome due to knowing whether someoneâ€™s annual income is above $50,000 or not.


# Exercise 3: Are the information gains in Exercises 2 and 3 like you would have expected? Why or why not? Could EDA have avoided any surprises here?


# Exercise 4: Perform k-means clustering on age, annual income, and the spending score. Explain your choice of k, visualize your results, and interpret the clusters. Make sure your analysis does not depend on the unit of measurements of the variables.

We start by subsetting the data and standardize each variable. Then we plot WSS against the number of clusters to determine a optimal value for k based on the plot. 
```{r}
kmdata <-  as.data.frame(data[c("Age", "Annual.Income", "Spending.Score")])
kmdata$Age <- kmdata$Age/(sd(kmdata$Age))
kmdata$Annual.Income <- kmdata$Annual.Income/(sd(kmdata$Annual.Income))
kmdata$Spending.Score <- kmdata$Spending.Score/(sd(kmdata$Spending.Score))

wss <- numeric(15)
for (k in 1:15) 
  wss[k] <- sum(kmeans(kmdata,centers=k, nstart=25)$withinss)
plot(1:15, wss, type="b", xlab="Number of Clusters", ylab="WSS")
```

k=4 seems to be the elbow of the WSS plot. So we choose k=4 to run the kmean analysis.  We run the kmean analysis below. 

```{r}
km <- kmeans(kmdata,4, nstart=25)
km
```

```{r}
library(ggplot2)
df = as.data.frame(kmdata)
df$cluster = factor(km$cluster)
centers=as.data.frame(km$centers)

g1 = ggplot(data=df, aes(x=Age, y=Annual.Income, color=cluster )) +
  geom_point() + theme(legend.position="right") +
  geom_point(data=centers,
             aes(x=Age,y=Annual.Income, color=as.factor(c(1,2,3,4))),
             size=10, alpha=.3, show.legend =FALSE)

g2 =ggplot(data=df, aes(x=Age, y=Spending.Score, color=cluster )) +
  geom_point() +
  geom_point(data=centers,
             aes(x=Age,y=Spending.Score, color=as.factor(c(1,2,3,4))),
             size=10, alpha=.3, show.legend=FALSE)

g3 = ggplot(data=df, aes(x=Annual.Income, y=Spending.Score, color=cluster )) +
  geom_point() +
  geom_point(data=centers,
             aes(x=Annual.Income,y=Spending.Score, color=as.factor(c(1,2,3,4))),
             size=10, alpha=.3, show.legend=FALSE)
g1
g2
g3
```


# Exercise 5: How does your k-means analysis compare to Section 3 (Segmentation using Age , Annual Income and Spending Score) from https://www.kaggle.com/kushal1996/customer-segmentation-k-means-analysis?

It show pretty different results, which mainly arise from a different choice of k. In this article, a k=6 is chosen, which is arguably inappropriate given that it is hardly the elbow of the inertia plot. Another difference is in terms of the visualization technique chosen: the article chose a  3D visualization with all three varibales, while ours entails 3 separate plots of 2 variables against each other. Arguably, ours is clearer and more helpful in helping us determine the meaning and significance of each cluster while the clustering shown in the article is confusing and even counterproductive since it is hard to make sense of the groupings just based off it. 


