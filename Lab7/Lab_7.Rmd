---
title: "Lab7"
author: "Dion, Julie, Mia, Vincent"
date: "9/20/2019"
output: 
    prettydoc::html_pretty:
    theme: cayman
    highlight: github
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
```


This lab considers the Kaggle dataset Mall Customer Segmentation Data https://www.kaggle.com/vjchoudhary7/customer-segmentation-tutorial-in-python which is posted as Mall_Customers.csv on Canvas.

For Exercises 1 through 3, consider whether the spending score is greater than 50 as outcome. Use the natural logarithm in your computations, not the log with base 2 like in the textbook. The R function log computes the natural logarithm by default.


```{r}
data <- read.csv("Mall_customers.csv")
names(data)[4:5] <- c("Annual.Income", "Spending.Score")
```



# Exercise 1: Compute the information gain in the outcome due to knowing whether someone is older than 36 or not.

First, we create a function that computes entropy, $H_Y$:
```{r}
entropy_func <- function(Y) {
  probs <- prop.table(table(Y))
  n <- length(probs)
  result <- rep_len(0, n)
  for (y in seq_len(n)) {
    result[y] <- probs[y] * log(probs[y])
  }
  -sum(result)
}

h_y <- entropy_func(data$Spending.Score > 50)
h_y
```


Now, we must compute conditional entropy, $H_{Y \vert X}$. To compute this, we first obtain the probability of the condition (someone being older than 36), $P(X)$.
```{r}
p_x <- table(data$Age > 36) / length(data$Age)
p_x
```

Thus, the probability of someone being older than 36, $P(X=1)$, equals 0.48 and the probability of someone being 36 and younger, $P(X=0)$, equals 0.52.

Then, we compute $P(Y\vert X=0)$ and $P(Y \vert X=1)$.

```{r}
p_ycondx <- c(
  data %>% # Y=1, X=1
    summarize(y1x1 = sum(Age > 36 & Spending.Score > 50) / sum(Age > 36)),
  data %>% # Y=0, X=1
    summarize(y0x1 = sum(Age > 36 & Spending.Score <= 50) / sum(Age > 36)),
  data %>% # Y=1, X=0
    summarize(y1x0 = sum(Age <= 36 & Spending.Score > 50) / sum(Age <= 36)),
  data %>% # Y=0, X=0
    summarize(y0x0 = sum(Age <= 36 & Spending.Score <= 50) / sum(Age <= 36)))
p_ycondx
```


Then, we apply the formula for conditional entropy, $$H_{Y \vert X}=-\sum_{\forall x \in X} P(x) \sum_{\forall y \in Y} P(y\vert x)\log P(y\vert x).$$

```{r}
h_ycondx <- -1*sum(0.52 * 
                     (p_ycondx[["y0x0"]] * log(p_ycondx[["y0x0"]]) +
                        p_ycondx[["y1x0"]] * log(p_ycondx[["y1x0"]])),
                   0.48 *
                     (p_ycondx[["y0x1"]] * log(p_ycondx[["y0x1"]]) +
                        p_ycondx[["y1x1"]] * log(p_ycondx[["y1x1"]])))
h_ycondx
```

Hence, the information gain is:
```{r}
h_y - h_ycondx
```




# Exercise 2: Compute the information gain in the outcome due to knowing whether someoneâ€™s annual income is above $50,000 or not.

We go through the same process as in Ex. 1, with the condition now being whether someone's annual income is above $50,000 or not. 

Computing $P(X)$:
```{r}
p_x2 <- table(data$Annual.Income > 50) / length(data$Annual.Income)
p_x2
```

Thus, the probability of someone's annual income being above \$50,000, $P(X=1)$, equals 0.63 and the probability of someone's annual income being below \$50,000, $P(X=0)$, equals 0.37.

Then, we compute $P(Y\vert X=0)$ and $P(Y \vert X=1)$.

```{r}
p_ycondx2 <- c(
  data %>% # Y=1, X=1
    summarize(y1x1 = sum(Annual.Income > 50 & Spending.Score > 50) / sum(Annual.Income > 50)),
  data %>% # Y=0, X=1
    summarize(y0x1 = sum(Annual.Income > 50 & Spending.Score <= 50) / sum(Annual.Income > 50)),
  data %>% # Y=1, X=0
    summarize(y1x0 = sum(Annual.Income <= 50 & Spending.Score > 50) / sum(Annual.Income <= 50)),
  data %>% # Y=0, X=0
    summarize(y0x0 = sum(Annual.Income <= 50 & Spending.Score <= 50) / sum(Annual.Income <= 50)))
p_ycondx2
```


Then, we apply the formula for conditional entropy:
```{r}
h_ycondx2 <- (-1)*sum(0.37 * 
                     (p_ycondx2[["y0x0"]] * log(p_ycondx2[["y0x0"]]) +
                        p_ycondx2[["y1x0"]] * log(p_ycondx2[["y1x0"]])),
                   0.63 *
                     (p_ycondx2[["y0x1"]] * log(p_ycondx2[["y0x1"]]) +
                        p_ycondx2[["y1x1"]] * log(p_ycondx2[["y1x1"]])))
h_ycondx2
```

Hence, the information gain is:
```{r}
h_y - h_ycondx2
```




# Exercise 3: Are the information gains in Exercises 2 and 3 like you would have expected? Why or why not? Could EDA have avoided any surprises here?

The information gain is higher when knowing whether someone is older than 36 (Ex. 1) than when knowing whether someone's annual income is higher than \$50,000 (Ex. 2). This is surprising because we expected spending score to have a greater correlation with annual income than with age, meaning that knowing someone's annual income should have given more information about someone's spending score. EDA could have avoided surprises here by first investigating the correlation between spending score and each of the predictors. 

```{r}
summary(lm(data$Spending.Score ~ data$Annual.Income))
```

```{r}
summary(lm(data$Spending.Score ~ data$Age))
```


As it turns out, our expectation of the correlation is wrong. In fact, spending score is more stongly linearly correlated with age (adjusted r-squared value of 0.1026) than with annual income (adjusted r-squared value of -0.004952).


# Exercise 4: Perform k-means clustering on age, annual income, and the spending score. Explain your choice of k, visualize your results, and interpret the clusters. Make sure your analysis does not depend on the unit of measurements of the variables.

We start by subsetting the data and standardize each variable. Then we plot WSS against the number of clusters to determine a optimal value for k based on the plot. 
```{r}
kmdata <-  as.data.frame(data[c("Age", "Annual.Income", "Spending.Score")])
kmdata$Age <- kmdata$Age/(sd(kmdata$Age))
kmdata$Annual.Income <- kmdata$Annual.Income/(sd(kmdata$Annual.Income))
kmdata$Spending.Score <- kmdata$Spending.Score/(sd(kmdata$Spending.Score))

wss <- numeric(15)
for (k in 1:15) 
  wss[k] <- sum(kmeans(kmdata,centers=k, nstart=25)$withinss)
plot(1:15, wss, type="b", xlab="Number of Clusters", ylab="WSS")
```

k=4 seems to be the elbow of the WSS plot. So we choose k=4 to run the kmean analysis.  We run the kmean analysis below. 

```{r}
km <- kmeans(kmdata,4, nstart=25)
km
```

```{r}
library(ggplot2)
df = as.data.frame(kmdata)
df$cluster = factor(km$cluster)
centers=as.data.frame(km$centers)

set.seed(1)

g1 = ggplot(data=df, aes(x=Age, y=Annual.Income, color=cluster )) +
  geom_point() + theme(legend.position="right") +
  geom_point(data=centers,
             aes(x=Age,y=Annual.Income, color=as.factor(c(1,2,3,4))),
             size=10, alpha=.3, show.legend =FALSE)

g2 = ggplot(data=df, aes(x=Age, y=Spending.Score, color=cluster )) +
  geom_point() +
  geom_point(data=centers,
             aes(x=Age,y=Spending.Score, color=as.factor(c(1,2,3,4))),
             size=10, alpha=.3, show.legend=FALSE)

g3 = ggplot(data=df, aes(x=Annual.Income, y=Spending.Score, color=cluster )) +
  geom_point() +
  geom_point(data=centers,
             aes(x=Annual.Income,y=Spending.Score, color=as.factor(c(1,2,3,4))),
             size=10, alpha=.3, show.legend=FALSE)
g1
g2
g3
```


# Exercise 5: How does your k-means analysis compare to Section 3 (Segmentation using Age , Annual Income and Spending Score) from https://www.kaggle.com/kushal1996/customer-segmentation-k-means-analysis?

It show pretty different results, which mainly arise from a different choice of k. In this article, a k=6 is chosen, which is arguably inappropriate given that it is hardly the elbow of the inertia plot. Another difference is in terms of the visualization technique chosen: the article chose a  3D visualization with all three varibales, while ours entails 3 separate plots of 2 variables against each other. Arguably, ours is clearer and more helpful in helping us determine the meaning and significance of each cluster while the clustering shown in the article is confusing and even counterproductive since it is hard to make sense of the groupings just based off it. 


