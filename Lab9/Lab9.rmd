---
title: "Lab9"
author: "Dion, Julie, Mia, Vincent"
date: "9/20/2019"
output: 
    prettydoc::html_pretty:
    theme: cayman
    highlight: github
---
```{r setup, include=FALSE}
install.packages("wordcloud") # word-cloud generator 
install.packages("RColorBrewer") # color palettes

knitr::opts_chunk$set(echo = TRUE)
library(knitr)       # used to make kable tables
library(tm)
library(SnowballC)
library(dplyr)
library(wordcloud)
data <- read.csv("ecommerce_women.csv")
head(data,5)
```

#Exericse 1: Take the review texts from the Womenâ€™s E-Commerce Clothing Reviews and turn them into a corpus. Change all upper case to lower case, remove punctuation, number and stopwords, stem the words, and delete any reviews with no text.

```{r}

# Create Corpus 
ccorpus<-Corpus(VectorSource(contentCorpus))
# Pre-process Corpus 
for (j in seq(contentCorpus)) {
    contentCorpus[[j]] <- gsub("/", " ", contentCorpus[[j]])
    contentCorpus[[j]] <- gsub("@", " ", contentCorpus[[j]])
    contentCorpus[[j]] <- gsub("\\|", " ", contentCorpus[[j]])
    contentCorpus[[j]] <- gsub("\u2028", " ", contentCorpus[[j]])
    }

ccorpus<-tm_map(ccorpus,tolower) 
ccorpus<-tm_map(ccorpus,removePunctuation)
ccorpus<-tm_map(ccorpus,removeNumbers)
ccorpus<-tm_map(ccorpus,removeWords,stopwords("en"))
ccleaned<-tm_map(ccorpus,stripWhitespace)
ccleaned <- tm_map(ccorpus, stemDocument)
head(ccleaned,5)
```

#Exercise 1 COntinued: DocumentTermMatrix(corpus) creates a document-term matrix with each entry counting how often the term corresponding with the column appears in the document corresponding with the row. DocumentTermMatrix does not return a standard matrix object, but one can transform it to matrix using as.matrix.



```{r}
# MATRIX
dtm <- TermDocumentMatrix(ccleaned)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
```

#Exercise 2: Make a histogram of the log of the document frequencies of all the terms in the corpus. For simplicity, let us constrain ourselves for the remainder of this lab to the 27 words that appear in no less than 10% and no more than 20% of all documents.

```{r}
library(ggplot2)

d <- d %>% mutate(log_freq = log(freq))

p <- ggplot(subset(d, log_freq>0), aes(x = reorder(word, -freq), y = freq)) +
          geom_bar(stat = "identity") + 
          theme(axis.text.x=element_text(angle=45, hjust=1))
p   
```

#Exercise 3: Create a word cloud of these 27 (stemmed) words where the size of the words is determined by their document frequency. Hint: Use Google if you do not know how to make a word cloud in R.

```{r}
min_terms <- findFreqTerms(dtm, lowfreq = 10)
min_m <- as.matrix(dtm)
min_v <- sort(rowSums(min_m),decreasing=TRUE)
min_d <- data.frame(word = names(min_v),freq=min_v)
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
          max.words=200, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))
```

#Exercise 4: Fit a logistic regression with as outcome whether the product is recommended by the reviewer. Use the TFIDF values with normalized term frequencies of the 27 words from the word cloud as predictors
```{r}

dtm_tfidf<- DocumentTermMatrix(ccleaned, control = list(weighting = weightTfIdf))

logit <- glm(data$tf_idf ~ ., family=binomial)
summary(logit)

```
## Exercise 5: Which (stemmed) words had a p-value of < 2e-16 in the logistic regression and seemed to indicate a negative review?



## Exericse 6: Plot an ROC curve for the logistic regression fit and compute its area under the curve.


```{r}
library(ROCR)
data_withprob <- data
data_withprob$PredictedProbability <- predict(logit, type="response")
predObj = prediction(data_withprob$PredictedProbability, data_withprob$SeriousDlqin2yrs)
rocObj = performance(predObj, measure="tpr", x.measure="fpr")
aucObj = performance(predObj, measure="auc")
plot(rocObj, main = paste("Area under the curve:",
round(aucObj@y.values[[1]] ,4)))

```






