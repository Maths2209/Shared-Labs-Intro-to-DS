---
title: "Lab9"
author: "Dion, Julie, Mia, Vincent"
date: "11/8/2019"
output: 
    prettydoc::html_pretty:
    theme: cayman
    highlight: github
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)       # used to make kable tables
library(tm)
library(SnowballC)
library(dplyr)
library(wordcloud)
library(RColorBrewer)
library(ggplot2)
```

# Exericse 1: Take the review texts from the Womenâ€™s E-Commerce Clothing Reviews and turn them into a corpus. Change all upper case to lower case, remove punctuation, number and stopwords, stem the words, and delete any reviews with no text.

```{r}
wc <- read.csv(file = "Womens Clothing E-Commerce Reviews.csv", stringsAsFactors = FALSE)

# Create Corpus
corpus <- VCorpus(VectorSource(wc$Review.Text))

clean_corpus <- function(corpus){
  corpus <- tm_map(corpus, content_transformer(tolower))
  corpus <- tm_map(corpus, removePunctuation)
  corpus <- tm_map(corpus, removeNumbers)
  corpus <- tm_map(corpus, removeWords, stopwords("en"))
  corpus <- tm_map(corpus, stemDocument)
  corpus <- tm_filter(corpus, FUN = function(x) content(x) != "")
  return(corpus)
}

ccleaned <- clean_corpus(corpus)
ccleaned
```

# Exercise 1 Continued: DocumentTermMatrix(corpus) creates a document-term matrix with each entry counting how often the term corresponding with the column appears in the document corresponding with the row. DocumentTermMatrix does not return a standard matrix object, but one can transform it to matrix using as.matrix.

```{r}
# MATRIX
dtm <- TermDocumentMatrix(ccleaned)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
```

#Exercise 2: Make a histogram of the log of the document frequencies of all the terms in the corpus. For simplicity, let us constrain ourselves for the remainder of this lab to the 27 words that appear in no less than 10% and no more than 20% of all documents.

```{r}
d <- d %>% mutate(log_freq = log(freq)) %>% mutate(perc_freq = freq / length(ccleaned) * 100)

p <- ggplot(d, aes(x = reorder(word, -freq), y = log_freq)) +
          geom_bar(stat = "identity") + 
          theme(axis.text.x=element_text(angle=45, hjust=1))
p   
```

#Exercise 3: Create a word cloud of these 27 (stemmed) words where the size of the words is determined by their document frequency. Hint: Use Google if you do not know how to make a word cloud in R.

```{r}
subset_words <- d[10 <= d$perc_freq & d$perc_freq <= 20,]
# min_terms <- findFreqTerms(dtm, lowfreq = 10)
# min_v <- sort(rowSums(m),decreasing=TRUE)
# min_d <- data.frame(word = names(min_v),freq=min_v)
set.seed(1234)
wordcloud(words = subset_words$word, freq = subset_words$freq,
          random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))
```

