---
title: "Lab5"
author: "Dion, Julie, Mia, Vincent"
date: "9/20/2019"
output: 
    prettydoc::html_pretty:
    theme: cayman
    highlight: github
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(readr)
library(rsq)
```

# Exercise 1: Consider logistic regression with target as the outcome. Perform backward elimination based on p-values using a significance level of 0.001 to obtain a parsimonious model for the presence of heart disease.

```{r}
heart <- read.csv('Lab_5_heart.csv',header=T,na.strings=c(""))
model <- glm(formula = heart$target ~ heart$age + heart$sex + heart$trestbps + heart$chol + heart$fbs + heart$thalach + heart$exang, family = binomial (link = "logit"))

summary(model)
```
Currently, the FBS predictor has the highest p-value of 0.6281. Using backward elimination, this predictor will be dropped in the next iteration of the model (model2).
```{r}
# REMOVE FBS

model2 <- glm(formula = heart$target ~ heart$age + heart$sex + heart$trestbps + heart$chol + heart$thalach + heart$exang, family = binomial(link = "logit"))

summary(model2)

```
The next predictor with the highest p-value appears to be age, with a p-value of 0.2249. This will also be dropped from the model. 

```{r}
# REMOVE AGE

model3 <- glm(formula = heart$target ~ heart$sex + heart$trestbps + heart$chol + heart$thalach + heart$exang, family = binomial(link = "logit"))

summary(model3)

```
At this point, chol seems to have the highest p-value and will be dropped. 

```{r}
# REMOVE CHOL 
model4 <- glm(formula = heart$target ~ heart$sex + heart$trestbps + heart$thalach + heart$exang, family = binomial(link = "logit"))

summary(model4)
```
At this point, trestbps seems to have the highest p-value (still >0.001) and will be dropped. 

```{r}
# REMOVE trestbps
model5 <- glm(formula = heart$target ~ heart$sex + heart$thalach + heart$exang, family = binomial(link = "logit"))

summary(model5)

```

# Excerise 2: Interpret the coefficient with the smallest p-value in the final model fit from Exercise 1. Make sure your interpretation is in context and explains the respective predictor.

In the final model, the predictor with the smalled p-value is the thalach predictor. This means that the thalach predictor is the most statistically significant predictor, and demonstrates that with each one unit increase in the target, there is a -1.564 decrease in the exang predictor. 

# Exercise 3: Create a side-by-side box plot of the predicted probabilities from your final model from Exercise 1 split out by whether the person had heart disease or not. What does this plot suggest about how well this model can determine whether someone has heart disease?

```{r}
heart$predict <- predict(model5, type = "response")
boxplot(predict ~ target, data = heart)
```

The plot shows a significant difference in the predicted probabilities of those with heart disease and without. Those without heart disease have a lower interquartile range and median, compared to that of those without the disease. This means that the model can generally predict whether someone has heart disease fairly well. However, there are some outliers in the boxplot for those who have heart disease, meaning that some of those individuals with heart disease had a low predicted probability. Still, the median predicted probability for those without heart disease is below 0.5 and the median for those with heart disease is above 0.5, indicating a well predicting model. 

# Exercise 4. Repeat Exercise 1 but now with forward selection rather than backward elimination. Again use a significance level of 0.001

 Model A(Age):
```{r}
modelA <- glm(formula = heart$target ~ heart$age, family = binomial(link = "logit")) 
summary(modelA)
``` 
Model B(Thalach):

```{r} 
modelB <- glm(formula = heart$target ~ heart$thalach, family = binomial(link = "logit"))
summary(modelB)
``` 

Model C(Exang): 
```{r}
modelC <- glm(formula = heart$target ~ heart$exang, family = binomial(link = "logit")) 
summary(modelC)
```
Model D: (Sex): 

```{r}
modelD <- glm(formula = heart$target ~ heart$sex, family = binomial(link = "logit"))
summary(modelD)
```
At this point, the exang predictor has the smallest p-value of 7.82e-13 and thus will be taken as the baseline model. 

1. Baseline Model (Exang):

```{r}
modelE <- glm(formula = heart$target ~ heart$exang, family = binomial(link = "logit"))
summary(modelE)
```

2. Model F (Exang+Thalach):

```{r}
modelF <- glm(formula = heart$target ~ heart$exang + heart$thalach, family = binomial(link = "logit"))
summary(modelF)
```

3. Model G (Exang+Sex):
```{r}
modelG <- glm(formula = heart$target ~ heart$exang + heart$sex, family = binomial(link = "logit"))
summary(modelG)
```
4. Model H (Exang+Age)
```{r}
modelH <- glm(formula = heart$target ~ heart$exang + heart$age, family = binomial(link = "logit"))
summary(modelH)
```
5. Model I (Exang+FBS)
```{r}
modelI <- glm(formula = heart$target ~ heart$exang + heart$fbs, family = binomial(link = "logit"))
summary(modelI)
```
6. Model J (Exang+Trest):
```{r}
modelJ <- glm(formula = heart$target ~ heart$exang + heart$trestbps, family = binomial(link = "logit"))
summary(modelJ)
```
Predictor thalach has the smallest p-value, which is 5.12e-07. Thus we can attach thalach as a predictor for the model. 

Then we try the next iteration:

```{r}
modelK <- glm(formula = heart$target ~ heart$exang + heart$thalach + heart$age, family = binomial(link = "logit"))
modelL <- glm(formula = heart$target ~ heart$exang + heart$thalach + heart$sex, family = binomial(link = "logit"))
modelM <- glm(formula = heart$target ~ heart$exang + heart$thalach + heart$trestbps, family = binomial(link = "logit"))
modelN <- glm(formula = heart$target ~ heart$exang + heart$thalach + heart$chol, family = binomial(link = "logit"))
modelO <- glm(formula = heart$target ~ heart$exang + heart$thalach + heart$fbs, family = binomial(link = "logit"))
summary(modelK)
summary(modelL)
summary(modelM)
summary(modelN)
summary(modelO)
```

Sex predictor has the least p-value and it's the only one <0.001 while the rest all have p values all >0.001. Hence, we can stop here with exang, thalach and sex as our three final predictors to be included in the model.


#Exercise 5: Are your final models from Exercises 1 and 4 the same? Are the final models from forward selection and backward elimination always the same? Please explain.

The model from backward elimination (Ex 1) and that from forward selection (Ex 4) are different. There is no guarantee that backward elimination and forward selection will arrive at the same final model. This is because stepwise regression methods are greedy in that you can add or remove one variable at a time. This process cannot yield the same model for all datasets. 

